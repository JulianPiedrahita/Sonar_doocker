2024.01.26 01:40:39 WARN  es[][o.e.t.ThreadPool] timer thread slept for [2h/7290910ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 03:33:01 WARN  es[][o.e.t.ThreadPool] timer thread slept for [1.8h/6712042ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 03:42:53 WARN  es[][o.e.t.ThreadPool] timer thread slept for [9.3m/562572ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 03:43:25 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5s/5003ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 03:43:26 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5s/5003596487ns] on relative clock which is above the warn threshold of [5000ms]
2024.01.26 04:43:57 WARN  es[][o.e.t.ThreadPool] timer thread slept for [1h/3631389ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 05:40:28 WARN  es[][o.e.t.ThreadPool] timer thread slept for [55.5m/3331492ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 05:45:19 WARN  es[][o.e.t.ThreadPool] timer thread slept for [4.3m/261651ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 06:46:21 WARN  es[][o.e.t.ThreadPool] timer thread slept for [1h/3631727ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 11:01:13 WARN  es[][o.e.t.ThreadPool] timer thread slept for [4.2h/15262182ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 13:02:14 WARN  es[][o.e.t.ThreadPool] timer thread slept for [2h/7231425ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 13:02:46 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5s/5018ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 13:02:46 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5s/5017816886ns] on relative clock which is above the warn threshold of [5000ms]
2024.01.26 13:53:12 WARN  es[][o.e.t.ThreadPool] timer thread slept for [50.4m/3025545ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 14:03:41 WARN  es[][o.e.t.ThreadPool] timer thread slept for [9.4m/569152ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 14:17:37 WARN  es[][o.e.t.ThreadPool] timer thread slept for [13.7m/822379ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 15:04:18 WARN  es[][o.e.t.ThreadPool] timer thread slept for [13.7m/822378155292ns] on relative clock which is above the warn threshold of [5000ms]
2024.01.26 15:04:18 WARN  es[][o.e.t.ThreadPool] timer thread slept for [46.6m/2801623ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 15:50:18 WARN  es[][o.e.t.ThreadPool] timer thread slept for [45.5m/2732257ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 15:50:18 WARN  es[][o.e.t.ThreadPool] timer thread slept for [45.5m/2732216863138ns] on relative clock which is above the warn threshold of [5000ms]
2024.01.26 16:04:45 WARN  es[][o.e.t.ThreadPool] timer thread slept for [8s/8014ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 16:04:45 WARN  es[][o.e.t.ThreadPool] timer thread slept for [8s/8013952649ns] on relative clock which is above the warn threshold of [5000ms]
2024.01.26 16:41:17 WARN  es[][o.e.t.ThreadPool] timer thread slept for [36.4m/2189349ms] on absolute clock which is above the warn threshold of [5000ms]
2024.01.26 17:03:23 INFO  es[][o.e.n.Node] stopping ...
2024.01.26 17:03:23 INFO  es[][o.e.n.Node] stopped
2024.01.26 17:03:23 INFO  es[][o.e.n.Node] closing ...
2024.01.26 17:03:23 INFO  es[][o.e.n.Node] closed
2024.01.26 17:04:24 INFO  es[][o.e.n.Node] version[7.17.8], pid[25], build[default/tar/120eabe1c8a0cb2ae87cffc109a5b65d213e9df1/2022-12-02T17:33:09.727072865Z], OS[Linux/6.5.11-linuxkit/amd64], JVM[Eclipse Adoptium/OpenJDK 64-Bit Server VM/17.0.7/17.0.7+7]
2024.01.26 17:04:24 INFO  es[][o.e.n.Node] JVM home [/opt/java/openjdk]
2024.01.26 17:04:24 INFO  es[][o.e.n.Node] JVM arguments [-XX:+UseG1GC, -Djava.io.tmpdir=/opt/sonarqube/temp, -XX:ErrorFile=/opt/sonarqube/logs/es_hs_err_pid%p.log, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djna.tmpdir=/opt/sonarqube/temp, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=COMPAT, -Dcom.redhat.fips=false, -Des.enforce.bootstrap.checks=true, -Xmx512m, -Xms512m, -XX:MaxDirectMemorySize=256m, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/opt/sonarqube/elasticsearch, -Des.path.conf=/opt/sonarqube/temp/conf/es, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=false]
2024.01.26 17:04:24 INFO  es[][o.e.p.PluginsService] loaded module [analysis-common]
2024.01.26 17:04:24 INFO  es[][o.e.p.PluginsService] loaded module [lang-painless]
2024.01.26 17:04:24 INFO  es[][o.e.p.PluginsService] loaded module [parent-join]
2024.01.26 17:04:24 INFO  es[][o.e.p.PluginsService] loaded module [reindex]
2024.01.26 17:04:24 INFO  es[][o.e.p.PluginsService] loaded module [transport-netty4]
2024.01.26 17:04:24 INFO  es[][o.e.p.PluginsService] no plugins loaded
2024.01.26 17:04:24 INFO  es[][o.e.e.NodeEnvironment] using [1] data paths, mounts [[/opt/sonarqube/data (/host_mark/Applications/Personal/Pragma/Sonar_doocker)]], net usable_space [279.6gb], net total_space [465.6gb], types [fakeowner]
2024.01.26 17:04:24 INFO  es[][o.e.e.NodeEnvironment] heap size [512mb], compressed ordinary object pointers [true]
2024.01.26 17:04:25 INFO  es[][o.e.n.Node] node name [sonarqube], node ID [-3slx_VnQWyjympej3Rg3g], cluster name [sonarqube], roles [data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
2024.01.26 17:04:29 INFO  es[][o.e.t.NettyAllocator] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=256kb, factors={es.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=1mb, heap_size=512mb}]
2024.01.26 17:04:29 INFO  es[][o.e.i.r.RecoverySettings] using rate limit [40mb] with [default=40mb, read=0b, write=0b, max=0b]
2024.01.26 17:04:29 INFO  es[][o.e.d.DiscoveryModule] using discovery type [zen] and seed hosts providers [settings]
2024.01.26 17:04:29 INFO  es[][o.e.g.DanglingIndicesState] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
2024.01.26 17:04:30 INFO  es[][o.e.n.Node] initialized
2024.01.26 17:04:30 INFO  es[][o.e.n.Node] starting ...
2024.01.26 17:04:30 INFO  es[][o.e.t.TransportService] publish_address {127.0.0.1:39717}, bound_addresses {127.0.0.1:39717}
2024.01.26 17:04:30 INFO  es[][o.e.b.BootstrapChecks] explicitly enforcing bootstrap checks
2024.01.26 17:04:30 INFO  es[][o.e.c.c.Coordinator] cluster UUID [VtLFwzn5Q5qqHzLnKuTjaA]
2024.01.26 17:04:30 INFO  es[][o.e.c.s.MasterService] elected-as-master ([1] nodes joined)[{sonarqube}{-3slx_VnQWyjympej3Rg3g}{TyB8mXvvSSWdI7q021WciA}{127.0.0.1}{127.0.0.1:39717}{cdfhimrsw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 4, version: 87, delta: master node changed {previous [], current [{sonarqube}{-3slx_VnQWyjympej3Rg3g}{TyB8mXvvSSWdI7q021WciA}{127.0.0.1}{127.0.0.1:39717}{cdfhimrsw}]}
2024.01.26 17:04:30 INFO  es[][o.e.c.s.ClusterApplierService] master node changed {previous [], current [{sonarqube}{-3slx_VnQWyjympej3Rg3g}{TyB8mXvvSSWdI7q021WciA}{127.0.0.1}{127.0.0.1:39717}{cdfhimrsw}]}, term: 4, version: 87, reason: Publication{term=4, version=87}
2024.01.26 17:04:31 INFO  es[][o.e.h.AbstractHttpServerTransport] publish_address {127.0.0.1:9001}, bound_addresses {127.0.0.1:9001}
2024.01.26 17:04:31 INFO  es[][o.e.n.Node] started
2024.01.26 17:04:31 INFO  es[][o.e.g.GatewayService] recovered [7] indices into cluster_state
2024.01.26 17:04:34 INFO  es[][o.e.c.r.a.AllocationService] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[metadatas][0]]]).
